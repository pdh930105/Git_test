{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mnist import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "def get_data():\n",
    "    (x_train, y_train), (x_test, y_test) = \\\n",
    "    load_mnist(normalize=True, flatten=True, one_hot_label=True)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "x_train, y_train, x_test, y_test = get_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [784, 30, 20, 10]\n",
    "W_dict = {}\n",
    "W_dict['W1'] = np.random.randn(784, 30) * 0.01\n",
    "W_dict['W2'] = np.random.randn(30, 20) * 0.01\n",
    "W_dict['W3'] = np.random.randn(20, 10) * 0.01\n",
    "\n",
    "x_tt = x_train[1:500]\n",
    "y_tt = y_train[1:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Softmax(ScoreMatrix):\n",
    "    if ScoreMatrix.ndim == 2:\n",
    "        temp = ScoreMatrix\n",
    "        temp = temp -np.max(temp, axis = 1, keepdims=True)\n",
    "        softmax_temp = np.exp(temp) / np.sum(np.exp(temp), axis=1, keepdims=True)\n",
    "        return softmax_temp\n",
    "    temp = ScoreMatrix\n",
    "    temp = temp -np.max(temp, axis = 0)\n",
    "    softmax_temp = np.exp(temp) / np.sum(np.exp(temp), axis=0)\n",
    "    return softmax_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    \n",
    "    def __init__(self, W, X):\n",
    "        self.W = W\n",
    "        self.X = X\n",
    "\n",
    "    \n",
    "    def computeScore(self):\n",
    "        self.Score = np.dot(self.X, self.W)\n",
    "        return self.Score\n",
    "    \n",
    "    def deltaX(self, dScore): # dS / dA -> \n",
    "        dActivation = np.dot(dScore, self.W.T)\n",
    "        return dActivation\n",
    "\n",
    "    def deltaW(self, dScore):\n",
    "        self.dW = np.dot(self.X.T, dScore)\n",
    "        return self.dW\n",
    "    \n",
    "    def backward(self, dScore):\n",
    "        dActivation = self.deltaX(dScore)\n",
    "        dW = self.deltaW(dScore)\n",
    "        return dW, dActivation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    \n",
    "    def __init__(self, Score):\n",
    "        self.Score = Score\n",
    "        self.mask = None\n",
    "    \n",
    "    def computeActivation(self):\n",
    "        self.mask = (self.Score <=0 )\n",
    "        self.Activation = self.Score.copy()\n",
    "        self.Activation[self.mask] = 0\n",
    "        return self.Activation\n",
    "    \n",
    "    def backward(self, dActivation): # dR / dS 실행 \n",
    "        dActivation[self.mask] = 0\n",
    "        dScore = dActivation\n",
    "        return dScore\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class two_hidden_layer:\n",
    "    \n",
    "    def __init__(self, X, W_dict):\n",
    "        self.X = X\n",
    "        self.layer_dict = {}\n",
    "        self.relu_dict = {}\n",
    "        self.W_dict = W_dict\n",
    "        self.Layer = {}\n",
    "    \n",
    "    def forward(self, X, Y):\n",
    "\n",
    "        LL1 = LinearLayer(self.W_dict['W1'], X)\n",
    "        Relu1 = Relu(LL1.computeScore())\n",
    "        Layer1 = (LL1, Relu1)\n",
    "        LL2 = LinearLayer(self.W_dict['W2'], Relu1.computeActivation())\n",
    "        Relu2 = Relu(LL2.computeScore())\n",
    "        Layer2 = (LL2, Relu2)\n",
    "        LL3 = LinearLayer(self.W_dict['W3'], Relu2.computeActivation())\n",
    "        Relu3 = Relu(LL3.computeScore())\n",
    "        Layer3 = (LL3, Relu3)\n",
    "        y_softmax = Softmax(Layer3[1].computeActivation())\n",
    "        \n",
    "        loss = -np.sum(Y * np.log(y_softmax)) / Y.shape[0]\n",
    "        \n",
    "        self.Layer = [Layer1, Layer2, Layer3]\n",
    "    \n",
    "        return loss, y_softmax\n",
    "    \n",
    "    def backward(self, y_softmax, Y):\n",
    "        #deltaLoss/deltaRelu3\n",
    "        dActivationLast = y_softmax - Y / Y.shape[0]\n",
    "        dLayer3Score = self.Layer[2][1].backward(dActivationLast)\n",
    "        dW3, dActivation3 = self.Layer[2][0].backward(dLayer3Score)\n",
    "        dLayer2Score = self.Layer[1][1].backward(dActivation3)\n",
    "        dW2, dActivation2 = self.Layer[1][0].backward(dLayer2Score)\n",
    "        dLayer1Score = self.Layer[0][1].backward(dActivation2)\n",
    "        dW1, dActivation1 = self.Layer[0][0].backward(dLayer1Score)\n",
    "        \n",
    "\n",
    "    def optimizer(self, x_train, y_train, x_test, y_test, learning_rate=0.1, epoch=1000):\n",
    "        for i in range(100):\n",
    "            loss, y_softmax = self.forward(x_train, y_train)\n",
    "            self.backward(y_softmax, y_train)\n",
    "            self.Layer[0][0].W -= learning_rate * self.Layer[0][0].dW\n",
    "            self.Layer[1][0].W -= learning_rate * self.Layer[1][0].dW\n",
    "            self.Layer[2][0].W -= learning_rate * self.Layer[2][0].dW\n",
    "            print(\"loss : \", loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  2.3025825790242744\n",
      "loss :  2.3025851029349163\n",
      "loss :  2.302585101758516\n",
      "loss :  2.3025851008384093\n",
      "loss :  2.3025850999196917\n",
      "loss :  2.302585099002172\n",
      "loss :  2.30258509808566\n",
      "loss :  2.302585097169966\n",
      "loss :  2.3025850962549\n",
      "loss :  2.302585095340274\n",
      "loss :  2.3025850944258974\n",
      "loss :  2.302585093511582\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n",
      "loss :  2.3025850929940455\n"
     ]
    }
   ],
   "source": [
    "NN = two_hidden_layer(x_tt, W_dict)\n",
    "\n",
    "NN.optimizer(x_tt, y_tt, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
